{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nimport time\n\nimport os\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#首先读取train文件，并保存为df_train数据框，进行描述性统计\ndf_train = pd.read_csv(\"../input/train.csv\")\ndf_train.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fe48342b-c503-40e5-a562-70b5aa11c887",
        "_uuid": "983eaf1f5e43a1e16c0d2649674964fd374de95d",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#然后读取train文件，并保存为df_test数据框，并进行描述性统计\ndf_test = pd.read_csv(\"../input/test.csv\")\ndf_test.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ad75791b-a14c-464b-986d-bd548a0692e4",
        "_uuid": "0a314490edb9febc276eb45ddd47fe3461bd9d1d",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#制作训练集，验证集和测试集\n#首先将数据按行随机打乱，然后进行切分，效果和train_test_split函数一样\n#注意要先进行打乱，再将X，Y分开，保证数据和标签一一对应\ndf_train = shuffle(df_train,random_state = 42)                             \n#用values可以讲数据框转换成不含有列名只含有内容的多维数组\ntrainXYorig = df_train.values\n#获取数据的行数\nm = trainXYorig.shape[0]\n#确定切分点，这里取行数的80%\npartition = int(m * 0.8)\n#将除开第一列标签的部分选取为列名，并且reshape成为四维数组，第一个维度是-1，代表的是样本数可以是任意张图片\ntrainXorig = trainXYorig[:, 1:].reshape(-1, 28, 28, 1)   #注意reshape是函数，因此接的是小括号\n#归一化\ntrainX = trainXorig / 255   \n#制作X的训练集和验证集，注意要先定义devX，否则trainX名字会更新，导致devX无法正常获取数据\ndevX = trainX[partition: , :, :, :]\ntrainX = trainX[0: partition, :, :, :]\n\n#取大矩阵的第一列为原始标签Y\ntrainYorig = trainXYorig[:, 0]\n#注意！！！将Y的标签从数字类别转化成独热向量，方便后面计算交叉熵，取10阶单位矩阵的第标签行作为新标签矩阵中对应的一行\ntrainY = np.eye(10)[trainYorig, :] \n#制作Y的训练集和验证集，同样的切分\ndevY = trainY[partition: , :]\ntrainY = trainY[0: partition, :]\n\n#测试集也要进行reshape\ntestXorig = df_test.values.reshape(-1, 28, 28, 1)\n#归一化\ntestX = testXorig / 255\n\n#输出各个数据集合的维数，查验是否出错\nprint(\"训练集X的形状是{} \".format(trainX.shape))\nprint(\"训练集Y的形状是{} \".format(trainY.shape))\nprint(\"验证集X的形状是{} \".format(devX.shape))\nprint(\"验证集Y的形状是{} \".format(devY.shape))\nprint(\"测试集X的形状是{} \".format(testX.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5fd17880-b930-4905-b138-367f8a59ce94",
        "_uuid": "b83cae865cf0cd34370aadb8220c2ceceb7b50ef",
        "collapsed": true,
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#画出一张图片，先选我们要花的第index张图片\nindex = 102\n#调用imshow的API，画出第index个样本的所有行，所有列，和第一个channel，cmap参数定义的是图片颜色，3channel图片无法定义cmap， interpolation可以定义缺失值插值方法（此处未用到）\nplt.imshow(trainX[index, :, :, 0], cmap ='gray')\n#给图片一个标题\nplt.title(\"No.{} picture,the label is {}\".format(index,trainYorig[index])) #标题中如果有中文无法正常显示？\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5ac32118-2084-42c0-8b7a-79322b6bde3b",
        "_uuid": "67fb4d0d553723eb5391178b669cc43880b9cd60",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#设置一个随机小批量梯度下降用的生成mini_batch的函数\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    #先获得输入数据的总行数\n    m = X.shape[0]                  \n    #定义一个空的mini-batch的list，用来放所有的mini_batch\n    mini_batches = []\n    #设置随机数种子\n    np.random.seed(seed)\n    \n    #permutation和shuffle类似，都是随机打乱，区别是permutation是返回一个被打乱的序列，原序列不变，shuffle直接改变原序列\n    #如果permutation的变量用的是一个integer，那么会直接打乱np.arange(m),如下所示，会得到一个乱序的值为0~m-1的list\n    permutation = list(np.random.permutation(m))\n    #得到一个打乱的X和Y，维数索引是前面的被打乱的m\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n    \n    #计算mini_batch的个数，mini_batch_size是函数里面给出的，int函数取整是取整数部分舍弃小数部分\n    num_complete_minibatches = int(m / mini_batch_size) \n    #对整个被打乱的X切片处理，并循环操作，得到一个很多mini_batch\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size: (k + 1) * mini_batch_size, :, :, :]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size: (k + 1) * mini_batch_size, :]\n        #每一个小batch都由打乱的X，Y拼成\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        #组合所有的小batch\n        mini_batches.append(mini_batch)\n    \n    #如果总的样本数不能被mini_batch的个数整除，将剩余的样本也补充进来\n    if (m % mini_batch_size != 0):\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :, :, :]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    #最后返回的是mini_batches的组合\n    return mini_batches",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4bfb46c6-f752-48d9-b7dd-24d256bf2ffe",
        "_uuid": "6018c9da6a89890b27aa5be8cb0306e126daff54"
      },
      "cell_type": "markdown",
      "source": "CNN"
    },
    {
      "metadata": {
        "_cell_guid": "5b4ee670-c605-4b15-9ef4-843083fb4fc8",
        "_uuid": "aab8009bf9c1d797009b2f365109e851d3ac124f",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 定义X，Y两个占位符\nX = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name = \"X\")\nY = tf.placeholder(dtype=tf.float32, shape=[None, 10], name = \"Y\")\n#定义一个布尔型变量的占位符，用来确定是不是在计算图需要训练的部分。\n#一张计算图里面根据你要fetch的值的不同，有些部分是不参与训练的。batch_norm层里面需要用到这个参数。对输出的均值和方差有影响。\nis_training = tf.placeholder(tf.bool)    \n#打印出来，确认X，Y的类型和维度是否正确\nprint(\"X = \" + str(X))\nprint(\"Y = \" + str(Y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "05d5444b-23eb-4b6f-a940-beb031f0f0e4",
        "_uuid": "9745fb8913dd0de03626225702cffadc9c9bcb56",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 定义W1，W2，W3三个参数，也就是卷积核；get_variable 和 variable 类似，有则调用，无则新建\n# leNet-5 只有两层卷积层，这里定义了三层，扩大了网络规模\n# 初始化方法采用的是泽维尔初始化，这种方法的好处是可以使得每层输出的方差大致相等，使得信息能够在网络中更好的流动，具体方法是在某个均匀分布中实现初始化\n#卷积核的前两个维度是单层卷积核的维度，也就是它的长和宽，第三个维度是上一层的channel的个数，第三个维度是卷积核总共的层数\nW1 = tf.get_variable(\"W1\", shape=[3, 3, 1, 4], initializer=tf.contrib.layers.xavier_initializer())  #每个卷积核都要定义名字，形状和初始化方法\nW2 = tf.get_variable(\"W2\", shape=[3, 3, 4, 8], initializer=tf.contrib.layers.xavier_initializer())\nW3 = tf.get_variable(\"W3\", shape=[3, 3, 8, 8], initializer=tf.contrib.layers.xavier_initializer()) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dde71f0a-5eb1-47f9-915a-42c2699f7961",
        "_uuid": "9f3a2e3c2ce5c5d199c11d0e62ea4c25de186c88",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 定义第一层网络结构，架构是:卷积运算 CONV2D(Z1) -> 批标准化 BatchNorm(N1) -> 激活函数 RELU(A1) -> 最大池化层 MAXPOOL(P1) -> \nZ1 = tf.nn.conv2d(input=X, filter=W1, strides=[1, 1, 1, 1], padding='SAME')    #conv2d 在tf.nn模块，需要定义输入，卷积核，步长和padding。padding分为same padding和valid padding 两种\nN1 = tf.layers.batch_normalization(Z1, training=is_training)      #批标准化在tf.layers模块！！！批标准化可以加速网络的计算\nA1 = tf.nn.relu(N1)   #对上一层的结果取激活函数为relu：max{x，0}\n#至此，卷积部分定义完毕，接下来是池化\nP1 = tf.nn.max_pool(value=A1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")  #池化层也需要padding；池化层的尺寸和步长设置貌似和卷积核不一样？\n\n# 定义第二层网络结构，架构是:卷积运算 CONV2D(Z2) -> 批标准化 BatchNorm(N2) -> 激活函数 RELU(A2) -> 最大池化层 MAXPOOL(P1) ->  \nZ2 = tf.nn.conv2d(input=P1, filter=W2, strides=[1, 1, 1, 1], padding='SAME')\nN2 = tf.layers.batch_normalization(Z2, training=is_training)\nA2 = tf.nn.relu(N2)\nP2 = tf.nn.max_pool(value=A2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n# 定义第三层网络结构，架构是:卷积运算 CONV2D(Z3) -> 批标准化 BatchNorm(N3) -> 激活函数 RELU(A3) -> 最大池化层 MAXPOOL(P1) -> \nZ3 = tf.nn.conv2d(input=P2, filter=W3, strides=[1, 1, 1, 1], padding='SAME')\nN3 = tf.layers.batch_normalization(Z3, training=is_training)\nA3 = tf.nn.relu(N3)\nP3 = tf.nn.max_pool(value=A3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n# 定义第四层网络结构，架构是:扁平化层 FLATTEN(F3) -> 全连接层 FULLYCONNECTED(Z4) -> 批标准化 BatchNorm(N4) -> 激活函数 RELU(A4)\nF3 = tf.contrib.layers.flatten(P3)    #contrib模块包含的是有试验性质的新方法\nZ4 = tf.contrib.layers.fully_connected(F3, num_outputs=64, activation_fn=None)    #总共要接两层全连接层，这是第一层，包含64个神经元，没有激活函数\nN4 = tf.layers.batch_normalization(Z4, training=is_training)    #批标准化\nA4 = tf.nn.relu(N4)\n\n# 定义第五层网络结构，仅含有一个全连接层做softmax层，64 to 10\nZ5 = tf.contrib.layers.fully_connected(A4, num_outputs=10, activation_fn=None)\n\n# 代价函数，reducemean就是对某一个维度求平均，可以搭配axis使用，没有axis的话，就是对所有维度求平均。\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z5, labels=Y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a24f3b75-4ae2-44f7-b9a4-9595df93660e",
        "_uuid": "5a9096fbb55602505b9f60242f3b15c9ca29e9f6",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 定义超参数\nlearning_rate = 0.002\nnum_epochs = 50\nmini_batch_size = 64\n# 初始化costs，为一个空列表\ncosts = []",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "89f527f7-b971-4562-968b-539a383ddf35",
        "_uuid": "98b6005d5c044d5807a99740faa423339109aec0",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 定义优化器\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n# 变量初始化\ninit = tf.global_variables_initializer()                                      ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "95d2eaef-6658-401e-b5f9-7ca6fc11320c",
        "_uuid": "d8143ae92ff9720743fce0b58dc9167dc7264cd5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 开启新会话，并且初始化\nsess = tf.Session()\nsess.run(init)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "65a8e8f6-0ea7-4182-b262-a09cc2df90b8",
        "_uuid": "93ee71e9a512fb9947807bf8cf78eb262e0b862d",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 第一层循环，从大周期epoch开始\nfor epoch in range(num_epochs):\n    # random_mini_batches函数，返回的是由很多mini_batch组成的mini_batches的列表\n    minibatches = random_mini_batches(trainX, trainY, mini_batch_size=mini_batch_size, seed=epoch + int(time.time()))\n    # 初始化epochcost\n    epoch_cost = 0.\n    \n    # 第二层循环，在minibatches列表内部循环\n    for minibatch in minibatches:\n        # ？？？\n        (minibatchX, minibatchY) = minibatch\n        # 不需要返回的值，可以赋值给空格；sess.run的参数，第一项是要fetch的值，第二项是feed_dict；计算上面定义的cost\n        _, mini_batch_cost, __ = sess.run([optimizer, cost, extra_update_ops], feed_dict={X: minibatchX, Y: minibatchY, is_training:True})\n        # 每一个epoch的cost等于所有mini_batch的cost相加\n        epoch_cost += mini_batch_cost\n\n    # 对epoch_cost 求平均\n    epoch_cost /= len(minibatches)\n    # 在costs列表 加上上一个epoch的cost\n    costs.append(epoch_cost)\n    # 如果 epoch是偶数（只输出一半的值）\n    if epoch % 2 == 0:\n        # 格式化输出，%d是数字，%f是浮点数\n        print(\"No. %d epoch, cost = %f\" % (epoch, epoch_cost))\n\n# 用plt画随着epoch变化costs的曲线\nplt.plot(costs)\nplt.xlabel(\"number of epochs\")\nplt.ylabel(\"cost\")\nplt.title(\"cost\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ec54aa81-c82d-46e6-b9bf-286fe0e6b279",
        "_uuid": "3a10f27b56d7306b4e0d8553c878fd8a65fdc9bf"
      },
      "cell_type": "markdown",
      "source": "Evaluation"
    },
    {
      "metadata": {
        "_cell_guid": "a9094741-3135-42bc-ba7e-36ead5cf178f",
        "_uuid": "a86782f32601b788a5c90422b548efcdd051c598",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# predict_op返回的是向量沿轴线方向的最大值的索引\npredict_op = tf.argmax(Z5, 1)\n# tf.equal 返回的是一个全是bool值变量的数组，相同返回True，不同返回False\ncorrect_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n# 将布尔型变化成浮点型，True为1，False为0.然后求平均，可以得到平均正确率\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d6c84bdc-4c2c-4274-9816-e49f3e323700",
        "_uuid": "309fb3d98140e52d108b6fe982c3e06d3bdb8303",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# eval()也是启动计算的一种方式。基于Tensorflow的基本原理，首先需要定义图，然后计算图，其中计算图的函数常见的有run()函数，如sess.run()。同样eval()也是此类函数，是sess.run()的另外一种写法。\n# 计算训练集误差\n# ？？？is_training标记为False的时候不会执行dropout等操作。注意is_training是放在feed_dict里面的\ntrain_accuracy = accuracy.eval(session=sess, feed_dict={X: trainX, Y: trainY, is_training:False})    \n# 计算验证集误差\ndev_accuracy = accuracy.eval(session=sess, feed_dict={X: devX, Y: devY, is_training:False})\n# 输出\nprint(\"train_accuracy = \" + str(train_accuracy))\nprint(\"dev_accuracy = \" + str(dev_accuracy))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "96077bc0-2e64-467e-bac8-9cc94e591a1f",
        "_uuid": "754cd8bbe015c1831b1e08197dc0f430e2574f84",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 计算测试集预测值\ntestY_pred = predict_op.eval(session=sess, feed_dict={X: testX, is_training:False})\n# 保存成一个一列dataframe，列标签是Label\ntestYDf = pd.DataFrame(testY_pred.reshape(-1, 1), index=np.arange(1, 1 + len(testY_pred)), columns=[\"Label\"]) # index 要求从 1 开始\ntestYDf.to_csv(\"test_predict.csv\", index=True, index_label=\"ImageId\")",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}